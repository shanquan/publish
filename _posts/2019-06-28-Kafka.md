---
layout: pagelt
title: Kafka trial
tags: [experience]
---

(test on 10.6.78.246)
Install docker-compose
```bash
ssh -R 8100:10.9.26.13:8080 root@10.6.78.246
curl -L "https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
chmod +x /usr/local/bin/docker-compose
docker-compose version
docker pull bitnami/kafka
docker pull dockerhub.azk8s.cn/bitnami/zookeeper
```

Start the server
```bash
curl -o docker-compose.yml -L https://github.com/bitnami/bitnami-docker-kafka/raw/master/docker-compose.yml
vi docker-compose.yml
#修改kafka和zookeeper镜像image tag为默认latest
docker-compose -f docker-compose.yml up -d
#退出：docker-compose -f docker-compose.yml down
#进入kafka container, or docker-compose exec with service name: `docker-compose exec kafka bash`
docker exec -it fafa7f6620dd bash
cd /opt/bitnami/kafka
#Step 3: Create a topic
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test
```

Setting up a multi-broker cluster
```bash
curl -o docker-compose-cluster.yml -L https://github.com/bitnami/bitnami-docker-kafka/raw/master/docker-compose-cluster.yml
vi docker-compose-cluster.yml
#修改kafka和zookeeper镜像image tag为默认latest，修改kafka和kafka2的volumn
#可选：清空volume数据，查看数据：docker inspect --format "{{.Config.Volumes}}" VolumeID
docker volume ls
docker volume prune
docker-compose -f docker-compose-cluster.yml up -d
docker-compose -f docker-compose-cluster.yml ps
#列出services
docker-compose -f docker-compose-cluster.yml config --services
docker-compose exec kafka bash
cd /opt/bitnami/kafka
#Now create a new topic with a replication factor of three:
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 3 --partitions 1 --topic my-replicated-topic
bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
#send & consume messages
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic
#kill the leader Broker
exit
docker ps | grep kafka
# kill leader's ContainerID
docker container stop 46e8ce6bfb24
docker-compose exec kafka bash
cd /opt/bitnami/kafka
bin/kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic my-replicated-topic
exit
docker container start 46e8ce6bfb24
docker-compose -f docker-compose-cluster.yml ps
```

Use Kafka Connect to import/export data
```bash
cd /home/kafka
docker-compose exec kafka bash
cd /opt/bitnami/kafka
echo -e "foo\nbar" > test.txt
# start two connectors running in standalone mode
bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
# see the sink file
more test.sink.txt
# start a consumer
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
> echo Another line>> test.txt
```

Use Kafka Streams to process data
```bash
cd /home/kafka
docker-compose exec kafka bash
cd /opt/bitnami/kafka
#Step 3: Prepare input topic and start Kafka producer
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-plaintext-input
bin/kafka-topics.sh --create \
    --bootstrap-server localhost:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic streams-wordcount-output \
    --config cleanup.policy=compact
bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe
#Step 4: Start the Wordcount Application
bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streams-plaintext-input
> bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \
    --topic streams-wordcount-output \
    --from-beginning \
    --formatter kafka.tools.DefaultMessageFormatter \
    --property print.key=true \
    --property print.value=true \
    --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
    --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
#Step 5: Process some data
#open producer console, input three messages one by one as below, and see outputs in consumer console
all streams lead to kafka
hello kafka streams
join kafka summit
```

refs:
- <http://kafka.apache.org/quickstart>
- <https://github.com/bitnami/bitnami-docker-kafka>
- <https://docs.confluent.io>
- <https://docs.confluent.io/current/quickstart/cos-docker-quickstart.html>
- <https://docs.confluent.io/current/installation/installing_cp/cp-helm-charts/docs/index.html#cp-helm-quickstart>
- <https://github.com/confluentinc/examples>